{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "np.random.seed(1671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training samples\n",
      "10000 testing samples\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size=128\n",
    "verbose=1\n",
    "nb_classes=10\n",
    "n_hidden=128\n",
    "validation_split=0.2\n",
    "reshaped=784\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "x_train=x_train.reshape(60000,reshaped)\n",
    "x_test=x_test.reshape(10000,reshaped)\n",
    "x_train=x_train.astype('float32')\n",
    "x_test=x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('{} training samples'.format(x_train.shape[0]))\n",
    "print('{} testing samples'.format(x_test.shape[0]))\n",
    "y_train=np_utils.to_categorical(y_train,nb_classes)\n",
    "y_test=np_utils.to_categorical(y_test,nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 1.4800 - acc: 0.6186 - mae: 0.1416 - val_loss: 0.7660 - val_acc: 0.8305 - val_mae: 0.0913\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.6065 - acc: 0.8480 - mae: 0.0716 - val_loss: 0.4598 - val_acc: 0.8809 - val_mae: 0.0543\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.4426 - acc: 0.8800 - mae: 0.0505 - val_loss: 0.3784 - val_acc: 0.8974 - val_mae: 0.0427\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3820 - acc: 0.8940 - mae: 0.0422 - val_loss: 0.3417 - val_acc: 0.9047 - val_mae: 0.0372\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3483 - acc: 0.9015 - mae: 0.0375 - val_loss: 0.3157 - val_acc: 0.9096 - val_mae: 0.0337\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.3250 - acc: 0.9071 - mae: 0.0345 - val_loss: 0.2984 - val_acc: 0.9150 - val_mae: 0.0314\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.3074 - acc: 0.9118 - mae: 0.0323 - val_loss: 0.2859 - val_acc: 0.9178 - val_mae: 0.0299\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2931 - acc: 0.9158 - mae: 0.0307 - val_loss: 0.2729 - val_acc: 0.9226 - val_mae: 0.0283\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2805 - acc: 0.9194 - mae: 0.0292 - val_loss: 0.2648 - val_acc: 0.9238 - val_mae: 0.0273\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2697 - acc: 0.9226 - mae: 0.0281 - val_loss: 0.2547 - val_acc: 0.9268 - val_mae: 0.0262\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2596 - acc: 0.9256 - mae: 0.0270 - val_loss: 0.2472 - val_acc: 0.9301 - val_mae: 0.0255\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2510 - acc: 0.9276 - mae: 0.0261 - val_loss: 0.2386 - val_acc: 0.9329 - val_mae: 0.0245\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2423 - acc: 0.9307 - mae: 0.0252 - val_loss: 0.2319 - val_acc: 0.9348 - val_mae: 0.0238\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2344 - acc: 0.9336 - mae: 0.0244 - val_loss: 0.2285 - val_acc: 0.9361 - val_mae: 0.0232\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2270 - acc: 0.9359 - mae: 0.0237 - val_loss: 0.2220 - val_acc: 0.9380 - val_mae: 0.0226\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2202 - acc: 0.9374 - mae: 0.0230 - val_loss: 0.2141 - val_acc: 0.9399 - val_mae: 0.0218\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2137 - acc: 0.9394 - mae: 0.0224 - val_loss: 0.2088 - val_acc: 0.9411 - val_mae: 0.0212\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2077 - acc: 0.9408 - mae: 0.0218 - val_loss: 0.2045 - val_acc: 0.9429 - val_mae: 0.0207\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.2020 - acc: 0.9428 - mae: 0.0212 - val_loss: 0.2001 - val_acc: 0.9440 - val_mae: 0.0203\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1965 - acc: 0.9441 - mae: 0.0207 - val_loss: 0.1958 - val_acc: 0.9456 - val_mae: 0.0199\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Test score 0.19413077054172753\n",
      "Test accuracy 0.9448999762535095\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(n_hidden,input_shape=(reshaped,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['acc','mae'])\n",
    "history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=verbose,validation_split=validation_split)\n",
    "score=model.evaluate(x_test,y_test)\n",
    "print('Test score {}'.format(score[0]))\n",
    "print('Test accuracy {}'.format(score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
